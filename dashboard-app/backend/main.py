#!/usr/bin/env python3
"""
Emergent Learning Dashboard - Backend API

FastAPI backend providing:
- REST API for dashboard data
- WebSocket for real-time updates
- Action endpoints (promote, retry, open in editor)
- Natural language query interface
- Workflow management

Run: uvicorn main:app --reload --port 8888
"""

import asyncio
import json
import logging
import os
import re
import sqlite3
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any
from contextlib import contextmanager
from collections import defaultdict
from dataclasses import asdict

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from starlette.middleware.base import BaseHTTPMiddleware

# Import session indexing
from session_index import SessionIndex

# Paths
EMERGENT_LEARNING_PATH = Path.home() / ".claude" / "emergent-learning"
DB_PATH = EMERGENT_LEARNING_PATH / "memory" / "index.db"
FRONTEND_PATH = Path(__file__).parent.parent / "frontend" / "dist"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Emergent Learning Dashboard",
    description="Interactive dashboard for AI agent orchestration and learning",
    version="1.0.0"
)

# CORS - restricted to local development origins only
# SECURITY: Since backend is localhost-only, this primarily prevents
# malicious websites from making requests if user visits them
ALLOWED_ORIGINS = [
    "http://localhost:3001",   # Vite dev server
    "http://localhost:8888",   # Backend serving frontend
    "http://127.0.0.1:3001",
    "http://127.0.0.1:8888",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["Content-Type", "Authorization"],
)


# ==============================================================================
# Security Headers Middleware
# ==============================================================================

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """Add security headers to all HTTP responses."""

    async def dispatch(self, request, call_next):
        response = await call_next(request)

        # Prevent clickjacking attacks
        response.headers["X-Frame-Options"] = "DENY"

        # Prevent MIME type sniffing
        response.headers["X-Content-Type-Options"] = "nosniff"

        # Enable XSS protection
        response.headers["X-XSS-Protection"] = "1; mode=block"

        # Control referrer information
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"

        # Restrict browser features
        response.headers["Permissions-Policy"] = "geolocation=(), microphone=(), camera=()"

        return response


app.add_middleware(SecurityHeadersMiddleware)


# ==============================================================================
# SQL Security
# ==============================================================================

def escape_like(s: str) -> str:
    """
    Escape SQL LIKE wildcards to prevent wildcard injection.

    Args:
        s: String to escape

    Returns:
        String with SQL LIKE wildcards escaped
    """
    return s.replace('\\', '\\\\').replace('%', '\\%').replace('_', '\\_')


# ==============================================================================
# Database Connection
# ==============================================================================

@contextmanager
def get_db():
    """Get database connection with row factory."""
    conn = sqlite3.connect(str(DB_PATH), timeout=10.0)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()


def dict_from_row(row) -> dict:
    """Convert sqlite3.Row to dict."""
    return dict(row) if row else None


# ==============================================================================
# Pydantic Models
# ==============================================================================

class HeuristicUpdate(BaseModel):
    rule: Optional[str] = None
    explanation: Optional[str] = None
    domain: Optional[str] = None
    is_golden: Optional[bool] = None


class DecisionCreate(BaseModel):
    title: str
    context: str
    options_considered: Optional[str] = None
    decision: str
    rationale: str
    domain: Optional[str] = None
    files_touched: Optional[str] = None
    tests_added: Optional[str] = None
    status: Optional[str] = "accepted"


class DecisionUpdate(BaseModel):
    title: Optional[str] = None
    context: Optional[str] = None
    options_considered: Optional[str] = None
    decision: Optional[str] = None
    rationale: Optional[str] = None
    domain: Optional[str] = None
    files_touched: Optional[str] = None
    tests_added: Optional[str] = None
    status: Optional[str] = None


class InvariantCreate(BaseModel):
    statement: str
    rationale: str
    domain: Optional[str] = None
    scope: Optional[str] = "codebase"  # codebase, module, function, runtime
    validation_type: Optional[str] = None  # manual, automated, test
    validation_code: Optional[str] = None
    severity: Optional[str] = "error"  # error, warning, info


class InvariantUpdate(BaseModel):
    statement: Optional[str] = None
    rationale: Optional[str] = None
    domain: Optional[str] = None
    scope: Optional[str] = None
    validation_type: Optional[str] = None
    validation_code: Optional[str] = None
    severity: Optional[str] = None
    status: Optional[str] = None


class AssumptionCreate(BaseModel):
    assumption: str
    context: str
    source: Optional[str] = None
    confidence: Optional[float] = 0.5
    domain: Optional[str] = None


class AssumptionUpdate(BaseModel):
    assumption: Optional[str] = None
    context: Optional[str] = None
    source: Optional[str] = None
    confidence: Optional[float] = None
    status: Optional[str] = None
    domain: Optional[str] = None


class WorkflowCreate(BaseModel):
    name: str
    description: str
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]


class QueryRequest(BaseModel):
    query: str
    limit: int = 20


class SpikeReportCreate(BaseModel):
    title: str
    topic: str
    question: str
    findings: str
    gotchas: Optional[str] = None
    resources: Optional[str] = None
    time_invested_minutes: Optional[int] = None
    domain: Optional[str] = None
    tags: Optional[str] = None


class SpikeReportUpdate(BaseModel):
    title: Optional[str] = None
    topic: Optional[str] = None
    question: Optional[str] = None
    findings: Optional[str] = None
    gotchas: Optional[str] = None
    resources: Optional[str] = None
    time_invested_minutes: Optional[int] = None
    domain: Optional[str] = None
    tags: Optional[str] = None


class SpikeReportRate(BaseModel):
    score: float  # 0-5 usefulness score


class ActionResult(BaseModel):
    success: bool
    message: str
    data: Optional[Dict] = None


# ==============================================================================
# WebSocket Manager for Real-Time Updates
# ==============================================================================

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.last_state_hash = None

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        dead_connections = []
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.warning(f"Failed to broadcast to client: {e}")
                dead_connections.append(connection)

        # Remove dead connections
        for conn in dead_connections:
            self.active_connections.remove(conn)

    async def broadcast_update(self, update_type: str, data: dict):
        await self.broadcast({
            "type": update_type,
            "timestamp": datetime.now().isoformat(),
            "data": data
        })


manager = ConnectionManager()

# Initialize session index
session_index = SessionIndex()


# ==============================================================================
# Background Task: Monitor for Changes
# ==============================================================================

async def monitor_changes():
    """Monitor database for changes and broadcast updates."""
    last_metrics_count = 0
    last_trail_count = 0
    last_run_count = 0
    last_heuristics_count = 0
    last_learnings_count = 0
    last_decisions_count = 0
    last_invariants_count = 0
    last_session_scan = None

    while True:
        try:
            with get_db() as conn:
                cursor = conn.cursor()

                # Check for new metrics
                cursor.execute("SELECT COUNT(*) FROM metrics")
                metrics_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM trails")
                trail_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM workflow_runs")
                run_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM heuristics")
                heuristics_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM learnings")
                learnings_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM decisions")
                decisions_count = cursor.fetchone()[0]

                cursor.execute("SELECT COUNT(*) FROM invariants")
                invariants_count = cursor.fetchone()[0]

                # Broadcast if changes detected
                if metrics_count > last_metrics_count:
                    # Get recent metrics
                    cursor.execute("""
                        SELECT metric_type, metric_name, metric_value, timestamp
                        FROM metrics
                        ORDER BY timestamp DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("metrics", {"recent": recent})
                    last_metrics_count = metrics_count

                if trail_count > last_trail_count:
                    cursor.execute("""
                        SELECT location, scent, strength, agent_id, message, created_at
                        FROM trails
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("trails", {"recent": recent})
                    last_trail_count = trail_count

                if run_count > last_run_count:
                    cursor.execute("""
                        SELECT id, workflow_name, status, phase, created_at
                        FROM workflow_runs
                        ORDER BY created_at DESC
                        LIMIT 1
                    """)
                    recent = dict_from_row(cursor.fetchone())
                    await manager.broadcast_update("runs", {"latest": recent})
                    last_run_count = run_count

                if heuristics_count > last_heuristics_count:
                    cursor.execute("""
                        SELECT id, domain, rule, confidence, is_golden, updated_at
                        FROM heuristics
                        ORDER BY updated_at DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("heuristics", {"recent": recent})
                    last_heuristics_count = heuristics_count

                if learnings_count > last_learnings_count:
                    cursor.execute("""
                        SELECT id, type, title, summary, domain, created_at
                        FROM learnings
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("learnings", {"recent": recent})
                    last_learnings_count = learnings_count

                if decisions_count > last_decisions_count:
                    cursor.execute("""
                        SELECT id, title, status, domain, created_at
                        FROM decisions
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("decisions", {"recent": recent})
                    last_decisions_count = decisions_count

                if invariants_count > last_invariants_count:
                    cursor.execute("""
                        SELECT id, statement, status, severity, domain, violation_count, created_at
                        FROM invariants
                        ORDER BY created_at DESC
                        LIMIT 5
                    """)
                    recent = [dict_from_row(r) for r in cursor.fetchall()]
                    await manager.broadcast_update("invariants", {"recent": recent})
                    last_invariants_count = invariants_count

            # Rescan session index every 5 minutes
            current_time = datetime.now()
            if last_session_scan is None or (current_time - last_session_scan).total_seconds() > 300:
                try:
                    session_count = session_index.scan()
                    logger.info(f"Session index refreshed: {session_count} sessions")
                    last_session_scan = current_time
                except Exception as e:
                    logger.error(f"Session index scan error: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"Monitor error: {e}", exc_info=True)

        await asyncio.sleep(2)  # Check every 2 seconds


@app.on_event("startup")
async def startup_event():
    # Initial session index scan
    try:
        session_count = session_index.scan()
        logger.info(f"Initial session index scan: {session_count} sessions")
    except Exception as e:
        logger.error(f"Failed to scan session index on startup: {e}", exc_info=True)

    # Start background monitoring
    asyncio.create_task(monitor_changes())


# ==============================================================================
# WebSocket Endpoint
# ==============================================================================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        # Send initial state
        await websocket.send_json({
            "type": "connected",
            "timestamp": datetime.now().isoformat(),
            "message": "Connected to Emergent Learning Dashboard"
        })

        while True:
            # Keep connection alive, handle any incoming messages
            try:
                data = await websocket.receive_text()
                if data == "ping":
                    await websocket.send_json({"type": "pong"})
            except Exception as e:
                logger.warning(f"WebSocket receive error: {e}")
                break

    except WebSocketDisconnect:
        logger.info("Client disconnected")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        manager.disconnect(websocket)


# ==============================================================================
# REST API: Dashboard Data
# ==============================================================================

@app.get("/api/stats")
async def get_stats():
    """Get overall statistics."""
    with get_db() as conn:
        cursor = conn.cursor()

        stats = {}

        # Counts
        cursor.execute("SELECT COUNT(*) FROM workflow_runs")
        stats["total_runs"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM node_executions")
        stats["total_executions"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM trails")
        stats["total_trails"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM heuristics")
        stats["total_heuristics"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM heuristics WHERE is_golden = 1")
        stats["golden_rules"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM learnings")
        stats["total_learnings"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM learnings WHERE type = 'failure'")
        stats["failures"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM learnings WHERE type = 'success'")
        stats["successes"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM decisions")
        stats["total_decisions"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM decisions WHERE status = 'accepted'")
        stats["accepted_decisions"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM decisions WHERE status = 'superseded'")
        stats["superseded_decisions"] = cursor.fetchone()[0]

        # Spike reports stats
        try:
            cursor.execute("SELECT COUNT(*) FROM spike_reports")
            stats["total_spike_reports"] = cursor.fetchone()[0]

            cursor.execute("SELECT AVG(usefulness_score) FROM spike_reports WHERE usefulness_score > 0")
            stats["avg_spike_usefulness"] = cursor.fetchone()[0] or 0

            cursor.execute("SELECT SUM(time_invested_minutes) FROM spike_reports")
            stats["total_spike_time_invested"] = cursor.fetchone()[0] or 0
        except Exception:
            stats["total_spike_reports"] = 0
            stats["avg_spike_usefulness"] = 0
            stats["total_spike_time_invested"] = 0

        # Get actual run success/failure counts from workflow_runs status
        cursor.execute("SELECT COUNT(*) FROM workflow_runs WHERE status = 'completed'")
        stats["successful_runs"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM workflow_runs WHERE status IN ('failed', 'cancelled')")
        stats["failed_runs"] = cursor.fetchone()[0]

        # Averages
        cursor.execute("SELECT AVG(confidence) FROM heuristics")
        stats["avg_confidence"] = cursor.fetchone()[0] or 0

        cursor.execute("SELECT SUM(times_validated) FROM heuristics")
        stats["total_validations"] = cursor.fetchone()[0] or 0

        cursor.execute("SELECT SUM(times_violated) FROM heuristics")
        stats["total_violations"] = cursor.fetchone()[0] or 0

        # Recent activity
        cursor.execute("""
            SELECT COUNT(*) FROM metrics
            WHERE timestamp > datetime('now', '-1 hour')
        """)
        stats["metrics_last_hour"] = cursor.fetchone()[0]

        # Runs today (actual workflow runs in last 24 hours)
        cursor.execute("""
            SELECT COUNT(*) FROM workflow_runs
            WHERE created_at > datetime('now', '-24 hours')
        """)
        stats["runs_today"] = cursor.fetchone()[0]

        # Query statistics
        cursor.execute("SELECT COUNT(*) FROM building_queries")
        stats["total_queries"] = cursor.fetchone()[0]

        cursor.execute("""
            SELECT COUNT(*) FROM building_queries
            WHERE created_at > datetime('now', '-24 hours')
        """)
        stats["queries_today"] = cursor.fetchone()[0]

        cursor.execute("""
            SELECT AVG(duration_ms) FROM building_queries
            WHERE duration_ms IS NOT NULL
        """)
        stats["avg_query_duration_ms"] = cursor.fetchone()[0] or 0


        # Invariant statistics
        cursor.execute("SELECT COUNT(*) FROM invariants")
        stats["total_invariants"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM invariants WHERE status = 'active'")
        stats["active_invariants"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM invariants WHERE status = 'violated'")
        stats["violated_invariants"] = cursor.fetchone()[0]

        cursor.execute("SELECT SUM(violation_count) FROM invariants")
        stats["total_invariant_violations"] = cursor.fetchone()[0] or 0

        return stats


@app.get("/api/heuristics")
async def get_heuristics(
    domain: Optional[str] = None,
    golden_only: bool = False,
    sort_by: str = "confidence",
    limit: int = 50
):
    """Get heuristics with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, domain, rule, explanation, confidence,
                   times_validated, times_violated, is_golden,
                   source_type, created_at, updated_at
            FROM heuristics
            WHERE 1=1
        """
        params = []

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if golden_only:
            query += " AND is_golden = 1"

        sort_map = {
            "confidence": "confidence DESC",
            "validated": "times_validated DESC",
            "violated": "times_violated DESC",
            "recent": "created_at DESC"
        }
        query += f" ORDER BY {sort_map.get(sort_by, 'confidence DESC')}"
        query += " LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/heuristics/{heuristic_id}")
async def get_heuristic(heuristic_id: int):
    """Get single heuristic with full details."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM heuristics WHERE id = ?
        """, (heuristic_id,))
        heuristic = dict_from_row(cursor.fetchone())

        if not heuristic:
            raise HTTPException(status_code=404, detail="Heuristic not found")

        # Get validation/violation history from metrics
        cursor.execute("""
            SELECT metric_type, timestamp, context
            FROM metrics
            WHERE tags LIKE ?
            ORDER BY timestamp DESC
            LIMIT 20
        """, (f"%heuristic_id:{heuristic_id}%",))
        heuristic["history"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Get related heuristics (same domain)
        cursor.execute("""
            SELECT id, rule, confidence
            FROM heuristics
            WHERE domain = ? AND id != ?
            ORDER BY confidence DESC
            LIMIT 5
        """, (heuristic["domain"], heuristic_id))
        heuristic["related"] = [dict_from_row(r) for r in cursor.fetchall()]

        return heuristic


@app.get("/api/heuristic-graph")
async def get_heuristic_graph():
    """Get heuristic graph data for force-directed visualization.

    Returns nodes (heuristics) and edges (relationships based on domain similarity
    and concept overlap).
    """
    with get_db() as conn:
        cursor = conn.cursor()

        # Get all heuristics with their key properties
        cursor.execute("""
            SELECT id, domain, rule, explanation, confidence,
                   times_validated, times_violated, is_golden,
                   created_at
            FROM heuristics
            ORDER BY confidence DESC
        """)

        heuristics = [dict_from_row(r) for r in cursor.fetchall()]

        # Create nodes
        nodes = []
        for h in heuristics:
            nodes.append({
                "id": h["id"],
                "label": h["rule"][:50] + ("..." if len(h["rule"]) > 50 else ""),
                "fullText": h["rule"],
                "domain": h["domain"],
                "confidence": h["confidence"],
                "is_golden": bool(h["is_golden"]),
                "times_validated": h["times_validated"],
                "times_violated": h["times_violated"],
                "explanation": h["explanation"],
                "created_at": h["created_at"]
            })

        # Create edges based on:
        # 1. Same domain (strong connection)
        # 2. Keyword overlap (weaker connection)
        edges = []
        edge_id = 0

        # Group heuristics by domain for same-domain connections
        domain_map = defaultdict(list)
        for h in heuristics:
            domain_map[h["domain"]].append(h["id"])

        # Create edges for same domain
        for domain, ids in domain_map.items():
            for i, id1 in enumerate(ids):
                for id2 in ids[i+1:]:
                    edges.append({
                        "id": edge_id,
                        "source": id1,
                        "target": id2,
                        "strength": 1.0,
                        "type": "same_domain",
                        "label": domain
                    })
                    edge_id += 1

        # Create edges for keyword similarity (limit to avoid too many edges)
        # Extract keywords from rules
        def extract_keywords(text):
            """Extract significant words from rule text."""
            if not text:
                return set()
            # Remove common words and extract significant terms
            stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'may', 'might', 'must', 'can'}
            words = re.findall(r'\w+', text.lower())
            return {w for w in words if len(w) > 3 and w not in stopwords}

        # Build keyword map
        heuristic_keywords = {}
        for h in heuristics:
            keywords = extract_keywords(h["rule"])
            if h["explanation"]:
                keywords.update(extract_keywords(h["explanation"]))
            heuristic_keywords[h["id"]] = keywords

        # Find keyword-based connections (only strong overlaps)
        for i, h1 in enumerate(heuristics):
            for h2 in heuristics[i+1:]:
                # Skip if same domain (already connected)
                if h1["domain"] == h2["domain"]:
                    continue

                keywords1 = heuristic_keywords[h1["id"]]
                keywords2 = heuristic_keywords[h2["id"]]

                # Calculate overlap
                overlap = keywords1 & keywords2
                if len(overlap) >= 2:  # At least 2 common keywords
                    strength = len(overlap) / max(len(keywords1), len(keywords2))
                    if strength > 0.2:  # Only strong connections
                        edges.append({
                            "id": edge_id,
                            "source": h1["id"],
                            "target": h2["id"],
                            "strength": strength,
                            "type": "keyword_similarity",
                            "label": ", ".join(list(overlap)[:3])
                        })
                        edge_id += 1

        # Limit edges per node to avoid clutter (keep strongest connections)
        MAX_EDGES_PER_NODE = 10
        node_edge_counts = defaultdict(list)
        for edge in edges:
            node_edge_counts[edge["source"]].append(edge)
            node_edge_counts[edge["target"]].append(edge)

        # Keep only top edges per node
        edges_to_keep = set()
        for node_id, node_edges in node_edge_counts.items():
            # Sort by strength and keep top N
            sorted_edges = sorted(node_edges, key=lambda e: e["strength"], reverse=True)
            for edge in sorted_edges[:MAX_EDGES_PER_NODE]:
                edges_to_keep.add(edge["id"])

        filtered_edges = [e for e in edges if e["id"] in edges_to_keep]

        return {
            "nodes": nodes,
            "edges": filtered_edges,
            "stats": {
                "total_nodes": len(nodes),
                "total_edges": len(filtered_edges),
                "golden_rules": sum(1 for n in nodes if n["is_golden"]),
                "domains": len(domain_map)
            }
        }


@app.get("/api/hotspots")
async def get_hotspots(days: int = 7, limit: int = 50):
    """Get trail hot spots aggregated by location."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                location,
                COUNT(*) as trail_count,
                SUM(strength) as total_strength,
                GROUP_CONCAT(DISTINCT scent) as scents,
                GROUP_CONCAT(DISTINCT agent_id) as agents,
                MAX(created_at) as last_activity,
                MIN(created_at) as first_activity
            FROM trails
            WHERE created_at > datetime('now', ?)
            GROUP BY location
            ORDER BY total_strength DESC
            LIMIT ?
        """, (f'-{days} days', limit))

        hotspots = []
        for row in cursor.fetchall():
            hs = dict_from_row(row)
            hs["scents"] = hs["scents"].split(",") if hs["scents"] else []
            hs["agents"] = hs["agents"].split(",") if hs["agents"] else []
            hs["agent_count"] = len(set(hs["agents"]))

            # Get related heuristics
            location_lower = hs["location"].lower()
            filename = location_lower.split("/")[-1].split("\\")[-1]
            base_name = filename.rsplit(".", 1)[0] if "." in filename else filename

            # Escape SQL wildcards to prevent wildcard injection
            base_name_escaped = escape_like(base_name)
            cursor.execute("""
                SELECT id, rule, confidence, domain
                FROM heuristics
                WHERE LOWER(rule) LIKE ? OR LOWER(domain) LIKE ?
                ORDER BY confidence DESC
                LIMIT 3
            """, (f'%{base_name_escaped}%', f'%{base_name_escaped}%'))
            hs["related_heuristics"] = [dict_from_row(r) for r in cursor.fetchall()]

            hotspots.append(hs)

        return hotspots


@app.get("/api/hotspots/treemap")
async def get_hotspots_treemap(days: int = 7):
    """Get hot spots formatted for treemap visualization."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                location,
                SUM(strength) as value,
                GROUP_CONCAT(DISTINCT scent) as scents,
                COUNT(*) as count
            FROM trails
            WHERE created_at > datetime('now', ?)
            GROUP BY location
        """, (f'-{days} days',))

        # Build tree structure
        root = {"name": "root", "children": []}
        dir_map = {}

        for row in cursor.fetchall():
            location = row["location"]
            value = row["value"] or 0
            scents = (row["scents"] or "").split(",")

            # Determine color based on scent
            color = "#60a5fa"  # default blue
            if "blocker" in scents:
                color = "#f85149"
            elif "warning" in scents:
                color = "#d29922"
            elif "discovery" in scents:
                color = "#3fb950"

            # Parse path
            parts = location.replace("\\", "/").split("/")
            current = root

            for i, part in enumerate(parts[:-1]):
                path = "/".join(parts[:i+1])
                if path not in dir_map:
                    new_node = {"name": part, "children": [], "path": path}
                    dir_map[path] = new_node
                    current["children"].append(new_node)
                current = dir_map[path]

            # Add leaf node
            current["children"].append({
                "name": parts[-1],
                "value": value,
                "color": color,
                "scents": scents,
                "count": row["count"],
                "path": location
            })

        return root


@app.get("/api/runs")
async def get_runs(days: int = 7, limit: int = 50, status: Optional[str] = None):
    """Get workflow runs."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, workflow_id, workflow_name, status, phase,
                   total_nodes, completed_nodes, failed_nodes,
                   started_at, completed_at, created_at
            FROM workflow_runs
            WHERE created_at > datetime('now', ?)
        """
        params = [f'-{days} days']

        if status:
            query += " AND status = ?"
            params.append(status)

        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/runs/{run_id}")
async def get_run(run_id: int):
    """Get single run with full details."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Get run
        cursor.execute("SELECT * FROM workflow_runs WHERE id = ?", (run_id,))
        run = dict_from_row(cursor.fetchone())

        if not run:
            raise HTTPException(status_code=404, detail="Run not found")

        # Get executions
        cursor.execute("""
            SELECT * FROM node_executions
            WHERE run_id = ?
            ORDER BY created_at
        """, (run_id,))
        run["executions"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Get trails
        cursor.execute("""
            SELECT * FROM trails
            WHERE run_id = ?
            ORDER BY created_at
        """, (run_id,))
        run["trails"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Get decisions
        cursor.execute("""
            SELECT * FROM conductor_decisions
            WHERE run_id = ?
            ORDER BY created_at
        """, (run_id,))
        run["decisions"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Get workflow edges if available
        if run.get("workflow_id"):
            cursor.execute("""
                SELECT * FROM workflow_edges
                WHERE workflow_id = ?
            """, (run["workflow_id"],))
            run["edges"] = [dict_from_row(r) for r in cursor.fetchall()]

        return run


@app.get("/api/runs/{run_id}/diff")
async def get_run_diff(run_id: int):
    """
    Get file changes (diff) for a run.

    NOTE: This currently returns mock/example data as the system doesn't yet
    track actual file content changes. In production, this would integrate with
    git or file system monitoring to show actual diffs.

    Future implementation: Track file snapshots before/after each run, or
    integrate with git to show actual diffs between commits.
    """
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify run exists
        cursor.execute("SELECT * FROM workflow_runs WHERE id = ?", (run_id,))
        run = cursor.fetchone()

        if not run:
            raise HTTPException(status_code=404, detail="Run not found")

        # Get trails (file locations touched) for this run
        cursor.execute("""
            SELECT DISTINCT location, scent, message
            FROM trails
            WHERE run_id = ? AND location_type = 'file'
            ORDER BY location
        """, (run_id,))
        trails = cursor.fetchall()

        # Generate mock diffs based on trails
        # TODO: Replace with actual file content tracking
        diffs = []
        for trail in trails:
            location = trail["location"]
            scent = trail["scent"]
            message = trail["message"] or "Modified file"

            # Create a realistic-looking mock diff
            if scent == "blocker":
                # Show a fix
                changes = [
                    {"type": "context", "lineNumber": 1, "oldLineNumber": 10, "newLineNumber": 10, "content": "def process_data(data):"},
                    {"type": "context", "lineNumber": 2, "oldLineNumber": 11, "newLineNumber": 11, "content": "    if not data:"},
                    {"type": "remove", "lineNumber": 3, "oldLineNumber": 12, "content": "        raise ValueError('Data is empty')"},
                    {"type": "add", "lineNumber": 4, "newLineNumber": 12, "content": "        logger.warning('Empty data received, using defaults')"},
                    {"type": "add", "lineNumber": 5, "newLineNumber": 13, "content": "        return get_default_data()"},
                    {"type": "context", "lineNumber": 6, "oldLineNumber": 13, "newLineNumber": 14, "content": "    return data"},
                ]
                diffs.append({
                    "path": location,
                    "changes": changes,
                    "additions": 2,
                    "deletions": 1
                })
            elif scent == "discovery":
                # Show new feature
                changes = [
                    {"type": "context", "lineNumber": 1, "oldLineNumber": 45, "newLineNumber": 45, "content": "class DataProcessor:"},
                    {"type": "add", "lineNumber": 2, "newLineNumber": 46, "content": "    def validate_input(self, data):"},
                    {"type": "add", "lineNumber": 3, "newLineNumber": 47, "content": "        \"\"\"Validate input data before processing.\"\"\""},
                    {"type": "add", "lineNumber": 4, "newLineNumber": 48, "content": "        if not isinstance(data, dict):"},
                    {"type": "add", "lineNumber": 5, "newLineNumber": 49, "content": "            raise TypeError('Data must be a dictionary')"},
                    {"type": "add", "lineNumber": 6, "newLineNumber": 50, "content": "        return True"},
                    {"type": "add", "lineNumber": 7, "newLineNumber": 51, "content": ""},
                    {"type": "context", "lineNumber": 8, "oldLineNumber": 46, "newLineNumber": 52, "content": "    def process(self, data):"},
                ]
                diffs.append({
                    "path": location,
                    "changes": changes,
                    "additions": 6,
                    "deletions": 0
                })
            else:
                # Generic change
                changes = [
                    {"type": "context", "lineNumber": 1, "oldLineNumber": 20, "newLineNumber": 20, "content": "# Configuration"},
                    {"type": "remove", "lineNumber": 2, "oldLineNumber": 21, "content": "DEBUG = True"},
                    {"type": "add", "lineNumber": 3, "newLineNumber": 21, "content": "DEBUG = False"},
                    {"type": "context", "lineNumber": 4, "oldLineNumber": 22, "newLineNumber": 22, "content": "TIMEOUT = 30"},
                ]
                diffs.append({
                    "path": location,
                    "changes": changes,
                    "additions": 1,
                    "deletions": 1
                })

        # If no trails found, return empty diffs with explanation
        if not diffs:
            # Check node_executions for any output that might indicate changes
            cursor.execute("""
                SELECT node_name, output
                FROM node_executions
                WHERE run_id = ? AND output IS NOT NULL
                LIMIT 5
            """, (run_id,))
            executions = cursor.fetchall()

            if executions:
                # Create synthetic diff from execution output
                for execution in executions:
                    changes = [
                        {"type": "context", "lineNumber": 1, "oldLineNumber": 1, "newLineNumber": 1, "content": f"# Changes from: {execution['node_name']}"},
                        {"type": "add", "lineNumber": 2, "newLineNumber": 2, "content": "# Output:"},
                        {"type": "add", "lineNumber": 3, "newLineNumber": 3, "content": str(execution['output'])[:100] + "..."},
                    ]
                    diffs.append({
                        "path": f"output/{execution['node_name']}.txt",
                        "changes": changes,
                        "additions": 2,
                        "deletions": 0
                    })

        return {
            "run_id": run_id,
            "diffs": diffs,
            "note": "This is mock data. Future versions will track actual file changes."
        }


@app.get("/api/timeline")
async def get_timeline(days: int = 7):
    """Get activity timeline data."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Runs by day
        cursor.execute("""
            SELECT DATE(created_at) as date, COUNT(*) as runs
            FROM workflow_runs
            WHERE created_at > datetime('now', ?)
            GROUP BY DATE(created_at)
            ORDER BY date
        """, (f'-{days} days',))
        runs_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Trails by day
        cursor.execute("""
            SELECT DATE(created_at) as date, COUNT(*) as trails, SUM(strength) as strength
            FROM trails
            WHERE created_at > datetime('now', ?)
            GROUP BY DATE(created_at)
            ORDER BY date
        """, (f'-{days} days',))
        trails_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Validations by day
        cursor.execute("""
            SELECT DATE(timestamp) as date, COUNT(*) as validations
            FROM metrics
            WHERE metric_type = 'heuristic_validated'
              AND timestamp > datetime('now', ?)
            GROUP BY DATE(timestamp)
            ORDER BY date
        """, (f'-{days} days',))
        validations_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Failures by day
        cursor.execute("""
            SELECT DATE(timestamp) as date, COUNT(*) as failures
            FROM metrics
            WHERE metric_type = 'auto_failure_capture'
              AND timestamp > datetime('now', ?)
            GROUP BY DATE(timestamp)
            ORDER BY date
        """, (f'-{days} days',))
        failures_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        return {
            "runs": runs_by_day,
            "trails": trails_by_day,
            "validations": validations_by_day,
            "failures": failures_by_day
        }


@app.get("/api/learning-velocity")
async def get_learning_velocity(days: int = 30):
    """Get learning velocity metrics over time."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Heuristics created per day
        cursor.execute("""
            SELECT DATE(created_at) as date, COUNT(*) as count
            FROM heuristics
            WHERE created_at > datetime('now', ?)
            GROUP BY DATE(created_at)
            ORDER BY date
        """, (f'-{days} days',))
        heuristics_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Learnings created per day
        cursor.execute("""
            SELECT DATE(created_at) as date,
                   COUNT(*) as total,
                   SUM(CASE WHEN type = 'failure' THEN 1 ELSE 0 END) as failures,
                   SUM(CASE WHEN type = 'success' THEN 1 ELSE 0 END) as successes
            FROM learnings
            WHERE created_at > datetime('now', ?)
            GROUP BY DATE(created_at)
            ORDER BY date
        """, (f'-{days} days',))
        learnings_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Golden rule promotions per day
        cursor.execute("""
            SELECT DATE(updated_at) as date, COUNT(*) as count
            FROM heuristics
            WHERE is_golden = 1
              AND updated_at > datetime('now', ?)
              AND updated_at > created_at
            GROUP BY DATE(updated_at)
            ORDER BY date
        """, (f'-{days} days',))
        promotions_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Confidence improvement rate - track average confidence over time
        cursor.execute("""
            SELECT DATE(updated_at) as date, AVG(confidence) as avg_confidence
            FROM heuristics
            WHERE updated_at > datetime('now', ?)
            GROUP BY DATE(updated_at)
            ORDER BY date
        """, (f'-{days} days',))
        confidence_by_day = [dict_from_row(r) for r in cursor.fetchall()]

        # Calculate weekly aggregates for trend analysis
        cursor.execute("""
            SELECT
                strftime('%Y-W%W', created_at) as week,
                COUNT(*) as heuristics_count
            FROM heuristics
            WHERE created_at > datetime('now', ?)
            GROUP BY week
            ORDER BY week
        """, (f'-{days} days',))
        heuristics_by_week = [dict_from_row(r) for r in cursor.fetchall()]

        # Learning streak - consecutive days with new heuristics or learnings
        cursor.execute("""
            WITH RECURSIVE dates(date) AS (
                SELECT DATE('now')
                UNION ALL
                SELECT DATE(date, '-1 day')
                FROM dates
                WHERE date > DATE('now', ?)
            ),
            activity AS (
                SELECT DISTINCT DATE(created_at) as date
                FROM (
                    SELECT created_at FROM heuristics
                    UNION ALL
                    SELECT created_at FROM learnings
                )
            )
            SELECT COUNT(*) as streak
            FROM dates d
            INNER JOIN activity a ON d.date = a.date
            WHERE d.date <= DATE('now')
            ORDER BY d.date DESC
        """, (f'-{days} days',))
        streak_result = cursor.fetchone()
        current_streak = streak_result[0] if streak_result else 0

        # Success/failure trend - ratio over time
        cursor.execute("""
            SELECT DATE(created_at) as date,
                   COUNT(*) as total,
                   CAST(SUM(CASE WHEN type = 'success' THEN 1 ELSE 0 END) AS FLOAT) /
                   CAST(COUNT(*) AS FLOAT) as success_ratio
            FROM learnings
            WHERE created_at > datetime('now', ?)
            GROUP BY DATE(created_at)
            HAVING total > 0
            ORDER BY date
        """, (f'-{days} days',))
        success_trend = [dict_from_row(r) for r in cursor.fetchall()]

        # Calculate velocity trends (% change week over week)
        heuristics_trend = 0.0
        if len(heuristics_by_week) >= 2:
            recent_week = heuristics_by_week[-1]['heuristics_count']
            prev_week = heuristics_by_week[-2]['heuristics_count']
            if prev_week > 0:
                heuristics_trend = ((recent_week - prev_week) / prev_week) * 100

        # Total stats for the period
        total_heuristics_period = sum(d['count'] for d in heuristics_by_day)
        total_learnings_period = sum(d['total'] for d in learnings_by_day)
        total_promotions_period = sum(d['count'] for d in promotions_by_day)

        return {
            "heuristics_by_day": heuristics_by_day,
            "learnings_by_day": learnings_by_day,
            "promotions_by_day": promotions_by_day,
            "confidence_by_day": confidence_by_day,
            "heuristics_by_week": heuristics_by_week,
            "success_trend": success_trend,
            "current_streak": current_streak,
            "heuristics_trend": round(heuristics_trend, 1),
            "totals": {
                "heuristics": total_heuristics_period,
                "learnings": total_learnings_period,
                "promotions": total_promotions_period
            }
        }


@app.get("/api/events")
async def get_events(limit: int = 50):
    """Get recent events feed."""
    with get_db() as conn:
        cursor = conn.cursor()

        events = []

        # Recent metrics (last hour)
        cursor.execute("""
            SELECT metric_type, metric_name, metric_value, tags, context, timestamp
            FROM metrics
            WHERE timestamp > datetime('now', '-1 hour')
            ORDER BY timestamp DESC
            LIMIT ?
        """, (limit,))

        for row in cursor.fetchall():
            r = dict_from_row(row)
            event_type = r["metric_type"]

            # Format event message
            if event_type == "heuristic_validated":
                message = f"Heuristic validated"
            elif event_type == "heuristic_violated":
                message = f"Heuristic violated"
            elif event_type == "auto_failure_capture":
                message = f"Failure auto-captured"
            elif event_type == "golden_rule_promotion":
                message = f"New golden rule promoted!"
            elif event_type == "task_outcome":
                message = f"Task {r['metric_name']}"
            else:
                message = f"{event_type}: {r['metric_name']}"

            events.append({
                "type": event_type,
                "message": message,
                "timestamp": r["timestamp"],
                "tags": r["tags"],
                "context": r["context"]
            })

        return events


@app.get("/api/learnings")
async def get_learnings(
    type: Optional[str] = None,
    domain: Optional[str] = None,
    limit: int = 50
):
    """Get learnings (failures, successes, observations)."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = "SELECT * FROM learnings WHERE 1=1"
        params = []

        if type:
            query += " AND type = ?"
            params.append(type)

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/decisions")
async def get_decisions(
    domain: Optional[str] = None,
    status: Optional[str] = None,
    skip: int = 0,
    limit: int = 50
):
    """Get architecture decisions with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, title, context, options_considered, decision, rationale,
                   domain, files_touched, tests_added, status, superseded_by,
                   created_at, updated_at
            FROM decisions
            WHERE 1=1
        """
        params = []

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if status:
            query += " AND status = ?"
            params.append(status)

        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.append(limit)
        params.append(skip)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/decisions/{decision_id}")
async def get_decision(decision_id: int):
    """Get single decision with full details."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM decisions WHERE id = ?
        """, (decision_id,))
        decision = dict_from_row(cursor.fetchone())

        if not decision:
            raise HTTPException(status_code=404, detail="Decision not found")

        # Get related decisions (same domain)
        cursor.execute("""
            SELECT id, title, status, created_at
            FROM decisions
            WHERE domain = ? AND id != ?
            ORDER BY created_at DESC
            LIMIT 5
        """, (decision["domain"], decision_id))
        decision["related"] = [dict_from_row(r) for r in cursor.fetchall()]

        # If this decision supersedes another, get the superseded decision
        if decision.get("superseded_by"):
            cursor.execute("""
                SELECT id, title, status
                FROM decisions
                WHERE id = ?
            """, (decision["superseded_by"],))
            decision["supersedes"] = dict_from_row(cursor.fetchone())

        # Get decisions that this one superseded
        cursor.execute("""
            SELECT id, title, status, created_at
            FROM decisions
            WHERE superseded_by = ?
        """, (decision_id,))
        decision["superseded_decisions"] = [dict_from_row(r) for r in cursor.fetchall()]

        return decision


@app.post("/api/decisions")
async def create_decision(decision: DecisionCreate) -> ActionResult:
    """Create a new architecture decision."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO decisions (
                title, context, options_considered, decision, rationale,
                domain, files_touched, tests_added, status, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            decision.title,
            decision.context,
            decision.options_considered,
            decision.decision,
            decision.rationale,
            decision.domain,
            decision.files_touched,
            decision.tests_added,
            decision.status,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))

        decision_id = cursor.lastrowid
        conn.commit()

        await manager.broadcast_update("decision_created", {
            "decision_id": decision_id,
            "title": decision.title,
            "domain": decision.domain
        })

        return ActionResult(
            success=True,
            message=f"Created decision: {decision.title}",
            data={"decision_id": decision_id}
        )


@app.put("/api/decisions/{decision_id}")
async def update_decision(decision_id: int, update: DecisionUpdate) -> ActionResult:
    """Update an existing decision."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify decision exists
        cursor.execute("SELECT id FROM decisions WHERE id = ?", (decision_id,))
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail="Decision not found")

        updates = []
        params = []

        if update.title is not None:
            updates.append("title = ?")
            params.append(update.title)

        if update.context is not None:
            updates.append("context = ?")
            params.append(update.context)

        if update.options_considered is not None:
            updates.append("options_considered = ?")
            params.append(update.options_considered)

        if update.decision is not None:
            updates.append("decision = ?")
            params.append(update.decision)

        if update.rationale is not None:
            updates.append("rationale = ?")
            params.append(update.rationale)

        if update.domain is not None:
            updates.append("domain = ?")
            params.append(update.domain)

        if update.files_touched is not None:
            updates.append("files_touched = ?")
            params.append(update.files_touched)

        if update.tests_added is not None:
            updates.append("tests_added = ?")
            params.append(update.tests_added)

        if update.status is not None:
            updates.append("status = ?")
            params.append(update.status)

        if not updates:
            return ActionResult(success=False, message="No updates provided")

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(decision_id)

        cursor.execute(f"""
            UPDATE decisions
            SET {", ".join(updates)}
            WHERE id = ?
        """, params)

        conn.commit()

        await manager.broadcast_update("decision_updated", {
            "decision_id": decision_id
        })

        return ActionResult(success=True, message="Decision updated")


@app.delete("/api/decisions/{decision_id}")
async def delete_decision(decision_id: int) -> ActionResult:
    """Delete a decision."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Check if decision exists
        cursor.execute("SELECT title FROM decisions WHERE id = ?", (decision_id,))
        decision = cursor.fetchone()
        if not decision:
            raise HTTPException(status_code=404, detail="Decision not found")

        # Delete the decision
        cursor.execute("DELETE FROM decisions WHERE id = ?", (decision_id,))
        conn.commit()

        await manager.broadcast_update("decision_deleted", {
            "decision_id": decision_id
        })

        return ActionResult(success=True, message=f"Deleted decision: {decision['title']}")


@app.post("/api/decisions/{decision_id}/supersede")
async def supersede_decision(decision_id: int, new_decision: DecisionCreate) -> ActionResult:
    """Supersede a decision with a new one."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify old decision exists
        cursor.execute("SELECT * FROM decisions WHERE id = ?", (decision_id,))
        old_decision = dict_from_row(cursor.fetchone())
        if not old_decision:
            raise HTTPException(status_code=404, detail="Decision not found")

        # Create new decision
        cursor.execute("""
            INSERT INTO decisions (
                title, context, options_considered, decision, rationale,
                domain, files_touched, tests_added, status, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            new_decision.title,
            new_decision.context,
            new_decision.options_considered,
            new_decision.decision,
            new_decision.rationale,
            new_decision.domain,
            new_decision.files_touched,
            new_decision.tests_added,
            new_decision.status,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))

        new_decision_id = cursor.lastrowid

        # Update old decision to mark it as superseded
        cursor.execute("""
            UPDATE decisions
            SET status = 'superseded', superseded_by = ?, updated_at = ?
            WHERE id = ?
        """, (new_decision_id, datetime.now().isoformat(), decision_id))

        conn.commit()

        await manager.broadcast_update("decision_superseded", {
            "old_decision_id": decision_id,
            "new_decision_id": new_decision_id,
            "title": new_decision.title
        })

        return ActionResult(
            success=True,
            message=f"Superseded decision #{decision_id} with #{new_decision_id}",
            data={"new_decision_id": new_decision_id, "old_decision_id": decision_id}
        )


# ==============================================================================
# REST API: Assumptions
# ==============================================================================

@app.get("/api/assumptions")
async def get_assumptions(
    domain: Optional[str] = None,
    status: Optional[str] = None,
    min_confidence: Optional[float] = None,
    skip: int = 0,
    limit: int = 50
):
    """Get assumptions with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, assumption, context, source, confidence, status, domain,
                   verified_count, challenged_count, last_verified_at,
                   created_at, updated_at
            FROM assumptions
            WHERE 1=1
        """
        params = []

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if status:
            query += " AND status = ?"
            params.append(status)

        if min_confidence is not None:
            query += " AND confidence >= ?"
            params.append(min_confidence)

        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.append(limit)
        params.append(skip)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/assumptions/{assumption_id}")
async def get_assumption(assumption_id: int):
    """Get single assumption with full details."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM assumptions WHERE id = ?
        """, (assumption_id,))
        assumption = dict_from_row(cursor.fetchone())

        if not assumption:
            raise HTTPException(status_code=404, detail="Assumption not found")

        # Get related assumptions (same domain)
        if assumption.get("domain"):
            cursor.execute("""
                SELECT id, assumption, status, confidence, created_at
                FROM assumptions
                WHERE domain = ? AND id != ?
                ORDER BY created_at DESC
                LIMIT 5
            """, (assumption["domain"], assumption_id))
            assumption["related"] = [dict_from_row(r) for r in cursor.fetchall()]
        else:
            assumption["related"] = []

        return assumption


@app.post("/api/assumptions")
async def create_assumption(assumption: AssumptionCreate) -> ActionResult:
    """Create a new assumption."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Validate confidence range
        confidence = assumption.confidence
        if confidence is None:
            confidence = 0.5
        confidence = max(0.0, min(1.0, confidence))

        cursor.execute("""
            INSERT INTO assumptions (
                assumption, context, source, confidence, domain,
                status, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, 'active', ?, ?)
        """, (
            assumption.assumption,
            assumption.context,
            assumption.source,
            confidence,
            assumption.domain,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))

        assumption_id = cursor.lastrowid
        conn.commit()

        await manager.broadcast_update("assumption_created", {
            "assumption_id": assumption_id,
            "assumption": assumption.assumption[:100],
            "domain": assumption.domain
        })

        return ActionResult(
            success=True,
            message=f"Created assumption #{assumption_id}",
            data={"assumption_id": assumption_id}
        )


@app.put("/api/assumptions/{assumption_id}")
async def update_assumption(assumption_id: int, update: AssumptionUpdate) -> ActionResult:
    """Update an existing assumption."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify assumption exists
        cursor.execute("SELECT id FROM assumptions WHERE id = ?", (assumption_id,))
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail="Assumption not found")

        updates = []
        params = []

        if update.assumption is not None:
            updates.append("assumption = ?")
            params.append(update.assumption)

        if update.context is not None:
            updates.append("context = ?")
            params.append(update.context)

        if update.source is not None:
            updates.append("source = ?")
            params.append(update.source)

        if update.confidence is not None:
            # Validate confidence range
            confidence = max(0.0, min(1.0, update.confidence))
            updates.append("confidence = ?")
            params.append(confidence)

        if update.status is not None:
            # Validate status
            valid_statuses = ['active', 'verified', 'challenged', 'invalidated']
            if update.status not in valid_statuses:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid status. Must be one of: {', '.join(valid_statuses)}"
                )
            updates.append("status = ?")
            params.append(update.status)

        if update.domain is not None:
            updates.append("domain = ?")
            params.append(update.domain)

        if not updates:
            return ActionResult(success=False, message="No updates provided")

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(assumption_id)

        cursor.execute(f"""
            UPDATE assumptions
            SET {", ".join(updates)}
            WHERE id = ?
        """, params)

        conn.commit()

        await manager.broadcast_update("assumption_updated", {
            "assumption_id": assumption_id
        })

        return ActionResult(success=True, message="Assumption updated")


@app.post("/api/assumptions/{assumption_id}/verify")
async def verify_assumption(assumption_id: int) -> ActionResult:
    """Mark an assumption as verified (increment verified_count)."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify assumption exists
        cursor.execute("SELECT * FROM assumptions WHERE id = ?", (assumption_id,))
        assumption = dict_from_row(cursor.fetchone())
        if not assumption:
            raise HTTPException(status_code=404, detail="Assumption not found")

        # Increment verified count and update confidence
        new_verified = assumption['verified_count'] + 1
        total_checks = new_verified + assumption['challenged_count']
        new_confidence = new_verified / total_checks if total_checks > 0 else assumption['confidence']

        cursor.execute("""
            UPDATE assumptions
            SET verified_count = ?,
                confidence = ?,
                status = CASE WHEN verified_count >= 3 THEN 'verified' ELSE status END,
                last_verified_at = ?,
                updated_at = ?
            WHERE id = ?
        """, (
            new_verified,
            new_confidence,
            datetime.now().isoformat(),
            datetime.now().isoformat(),
            assumption_id
        ))

        conn.commit()

        await manager.broadcast_update("assumption_verified", {
            "assumption_id": assumption_id,
            "verified_count": new_verified,
            "new_confidence": new_confidence
        })

        return ActionResult(
            success=True,
            message=f"Assumption verified ({new_verified} times)",
            data={"verified_count": new_verified, "confidence": new_confidence}
        )


@app.post("/api/assumptions/{assumption_id}/challenge")
async def challenge_assumption(assumption_id: int) -> ActionResult:
    """Mark an assumption as challenged (increment challenged_count)."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify assumption exists
        cursor.execute("SELECT * FROM assumptions WHERE id = ?", (assumption_id,))
        assumption = dict_from_row(cursor.fetchone())
        if not assumption:
            raise HTTPException(status_code=404, detail="Assumption not found")

        # Increment challenged count and update confidence
        new_challenged = assumption['challenged_count'] + 1
        total_checks = assumption['verified_count'] + new_challenged
        new_confidence = assumption['verified_count'] / total_checks if total_checks > 0 else assumption['confidence'] * 0.8

        # Determine new status
        new_status = assumption['status']
        if new_challenged >= 3 and assumption['verified_count'] == 0:
            new_status = 'invalidated'
        elif new_challenged >= assumption['verified_count']:
            new_status = 'challenged'

        cursor.execute("""
            UPDATE assumptions
            SET challenged_count = ?,
                confidence = ?,
                status = ?,
                updated_at = ?
            WHERE id = ?
        """, (
            new_challenged,
            new_confidence,
            new_status,
            datetime.now().isoformat(),
            assumption_id
        ))

        conn.commit()

        await manager.broadcast_update("assumption_challenged", {
            "assumption_id": assumption_id,
            "challenged_count": new_challenged,
            "new_confidence": new_confidence,
            "new_status": new_status
        })

        return ActionResult(
            success=True,
            message=f"Assumption challenged ({new_challenged} times, status: {new_status})",
            data={"challenged_count": new_challenged, "confidence": new_confidence, "status": new_status}
        )


@app.delete("/api/assumptions/{assumption_id}")
async def delete_assumption(assumption_id: int) -> ActionResult:
    """Delete an assumption."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Check if assumption exists
        cursor.execute("SELECT assumption FROM assumptions WHERE id = ?", (assumption_id,))
        assumption = cursor.fetchone()
        if not assumption:
            raise HTTPException(status_code=404, detail="Assumption not found")

        # Delete the assumption
        cursor.execute("DELETE FROM assumptions WHERE id = ?", (assumption_id,))
        conn.commit()

        await manager.broadcast_update("assumption_deleted", {
            "assumption_id": assumption_id
        })

        return ActionResult(success=True, message=f"Deleted assumption #{assumption_id}")


# ==============================================================================
# REST API: Invariants
# ==============================================================================

@app.get("/api/invariants")
async def get_invariants(
    domain: Optional[str] = None,
    scope: Optional[str] = None,
    status: Optional[str] = None,
    severity: Optional[str] = None,
    skip: int = 0,
    limit: int = 50
):
    """Get invariants with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, statement, rationale, domain, scope, validation_type,
                   validation_code, severity, status, violation_count,
                   last_validated_at, last_violated_at, created_at, updated_at
            FROM invariants
            WHERE 1=1
        """
        params = []

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if scope:
            query += " AND scope = ?"
            params.append(scope)

        if status:
            query += " AND status = ?"
            params.append(status)

        if severity:
            query += " AND severity = ?"
            params.append(severity)

        query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
        params.append(limit)
        params.append(skip)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/invariants/{invariant_id}")
async def get_invariant(invariant_id: int):
    """Get single invariant with full details."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM invariants WHERE id = ?
        """, (invariant_id,))
        invariant = dict_from_row(cursor.fetchone())

        if not invariant:
            raise HTTPException(status_code=404, detail="Invariant not found")

        # Get related invariants (same domain)
        if invariant.get("domain"):
            cursor.execute("""
                SELECT id, statement, status, severity, created_at
                FROM invariants
                WHERE domain = ? AND id != ?
                ORDER BY created_at DESC
                LIMIT 5
            """, (invariant["domain"], invariant_id))
            invariant["related"] = [dict_from_row(r) for r in cursor.fetchall()]
        else:
            invariant["related"] = []

        return invariant


@app.post("/api/invariants")
async def create_invariant(invariant: InvariantCreate) -> ActionResult:
    """Create a new invariant."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO invariants (
                statement, rationale, domain, scope, validation_type,
                validation_code, severity, status, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, 'active', ?, ?)
        """, (
            invariant.statement,
            invariant.rationale,
            invariant.domain,
            invariant.scope,
            invariant.validation_type,
            invariant.validation_code,
            invariant.severity,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))

        invariant_id = cursor.lastrowid
        conn.commit()

        await manager.broadcast_update("invariant_created", {
            "invariant_id": invariant_id,
            "statement": invariant.statement[:100],
            "domain": invariant.domain
        })

        return ActionResult(
            success=True,
            message=f"Created invariant: {invariant.statement[:50]}...",
            data={"invariant_id": invariant_id}
        )


@app.put("/api/invariants/{invariant_id}")
async def update_invariant(invariant_id: int, update: InvariantUpdate) -> ActionResult:
    """Update an existing invariant."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify invariant exists
        cursor.execute("SELECT id FROM invariants WHERE id = ?", (invariant_id,))
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail="Invariant not found")

        updates = []
        params = []

        if update.statement is not None:
            updates.append("statement = ?")
            params.append(update.statement)

        if update.rationale is not None:
            updates.append("rationale = ?")
            params.append(update.rationale)

        if update.domain is not None:
            updates.append("domain = ?")
            params.append(update.domain)

        if update.scope is not None:
            updates.append("scope = ?")
            params.append(update.scope)

        if update.validation_type is not None:
            updates.append("validation_type = ?")
            params.append(update.validation_type)

        if update.validation_code is not None:
            updates.append("validation_code = ?")
            params.append(update.validation_code)

        if update.severity is not None:
            updates.append("severity = ?")
            params.append(update.severity)

        if update.status is not None:
            updates.append("status = ?")
            params.append(update.status)

        if not updates:
            return ActionResult(success=False, message="No updates provided")

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(invariant_id)

        cursor.execute(f"""
            UPDATE invariants
            SET {", ".join(updates)}
            WHERE id = ?
        """, params)

        conn.commit()

        await manager.broadcast_update("invariant_updated", {
            "invariant_id": invariant_id
        })

        return ActionResult(success=True, message="Invariant updated")


@app.post("/api/invariants/{invariant_id}/validate")
async def validate_invariant(invariant_id: int) -> ActionResult:
    """Mark an invariant as validated (update last_validated_at)."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify invariant exists
        cursor.execute("SELECT statement FROM invariants WHERE id = ?", (invariant_id,))
        invariant = cursor.fetchone()
        if not invariant:
            raise HTTPException(status_code=404, detail="Invariant not found")

        cursor.execute("""
            UPDATE invariants
            SET last_validated_at = ?, updated_at = ?
            WHERE id = ?
        """, (datetime.now().isoformat(), datetime.now().isoformat(), invariant_id))

        conn.commit()

        await manager.broadcast_update("invariant_validated", {
            "invariant_id": invariant_id
        })

        return ActionResult(
            success=True,
            message=f"Invariant #{invariant_id} marked as validated",
            data={"invariant_id": invariant_id, "validated_at": datetime.now().isoformat()}
        )


@app.post("/api/invariants/{invariant_id}/violate")
async def record_invariant_violation(invariant_id: int) -> ActionResult:
    """Record a violation of an invariant (increment count, update timestamp)."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Verify invariant exists
        cursor.execute("SELECT statement, violation_count FROM invariants WHERE id = ?", (invariant_id,))
        invariant = cursor.fetchone()
        if not invariant:
            raise HTTPException(status_code=404, detail="Invariant not found")

        new_count = (invariant["violation_count"] or 0) + 1

        cursor.execute("""
            UPDATE invariants
            SET violation_count = ?,
                last_violated_at = ?,
                updated_at = ?,
                status = CASE WHEN ? >= 3 THEN 'violated' ELSE status END
            WHERE id = ?
        """, (new_count, datetime.now().isoformat(), datetime.now().isoformat(), new_count, invariant_id))

        conn.commit()

        await manager.broadcast_update("invariant_violated", {
            "invariant_id": invariant_id,
            "violation_count": new_count
        })

        return ActionResult(
            success=True,
            message=f"Recorded violation for invariant #{invariant_id} (total: {new_count})",
            data={"invariant_id": invariant_id, "violation_count": new_count}
        )


@app.delete("/api/invariants/{invariant_id}")
async def delete_invariant(invariant_id: int) -> ActionResult:
    """Delete an invariant."""
    with get_db() as conn:
        cursor = conn.cursor()

        # Check if invariant exists
        cursor.execute("SELECT statement FROM invariants WHERE id = ?", (invariant_id,))
        invariant = cursor.fetchone()
        if not invariant:
            raise HTTPException(status_code=404, detail="Invariant not found")

        # Delete the invariant
        cursor.execute("DELETE FROM invariants WHERE id = ?", (invariant_id,))
        conn.commit()

        await manager.broadcast_update("invariant_deleted", {
            "invariant_id": invariant_id
        })

        return ActionResult(success=True, message=f"Deleted invariant: {invariant['statement'][:50]}...")


# ==============================================================================
# REST API: Spike Reports
# ==============================================================================

@app.get("/api/spike-reports")
async def get_spike_reports(
    domain: Optional[str] = None,
    tags: Optional[str] = None,
    search: Optional[str] = None,
    sort_by: str = "recent",
    skip: int = 0,
    limit: int = 50
):
    """Get spike reports with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, title, topic, question, findings, gotchas, resources,
                   time_invested_minutes, domain, tags, usefulness_score,
                   access_count, created_at, updated_at
            FROM spike_reports
            WHERE 1=1
        """
        params = []

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if tags:
            tag_list = [t.strip() for t in tags.split(',')]
            tag_conditions = " OR ".join(["tags LIKE ?" for _ in tag_list])
            query += f" AND ({tag_conditions})"
            params.extend([f"%{escape_like(tag)}%" for tag in tag_list])

        if search:
            escaped_search = escape_like(search)
            query += " AND (title LIKE ? OR topic LIKE ? OR question LIKE ? OR findings LIKE ?)"
            params.extend([f"%{escaped_search}%"] * 4)

        sort_map = {
            "recent": "created_at DESC",
            "useful": "usefulness_score DESC",
            "accessed": "access_count DESC",
            "time": "time_invested_minutes DESC"
        }
        query += f" ORDER BY {sort_map.get(sort_by, 'created_at DESC')}"
        query += " LIMIT ? OFFSET ?"
        params.append(limit)
        params.append(skip)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/spike-reports/search")
async def search_spike_reports(
    q: str = Query(..., min_length=2),
    limit: int = 20
):
    """Full-text search in spike report findings and gotchas."""
    with get_db() as conn:
        cursor = conn.cursor()

        try:
            cursor.execute("""
                SELECT s.id, s.title, s.topic, s.question, s.findings, s.gotchas,
                       s.resources, s.time_invested_minutes, s.domain, s.tags,
                       s.usefulness_score, s.access_count, s.created_at, s.updated_at
                FROM spike_reports s
                JOIN spike_reports_fts fts ON fts.spike_id = s.id
                WHERE spike_reports_fts MATCH ?
                ORDER BY s.usefulness_score DESC
                LIMIT ?
            """, (q, limit))
            results = [dict_from_row(r) for r in cursor.fetchall()]
            if results:
                return results
        except Exception as e:
            logger.debug(f"FTS search failed, falling back to LIKE: {e}")

        escaped_q = escape_like(q)
        cursor.execute("""
            SELECT id, title, topic, question, findings, gotchas, resources,
                   time_invested_minutes, domain, tags, usefulness_score,
                   access_count, created_at, updated_at
            FROM spike_reports
            WHERE title LIKE ? OR topic LIKE ? OR question LIKE ?
               OR findings LIKE ? OR gotchas LIKE ?
            ORDER BY usefulness_score DESC
            LIMIT ?
        """, tuple([f"%{escaped_q}%"] * 5) + (limit,))

        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/spike-reports/{spike_id}")
async def get_spike_report(spike_id: int):
    """Get single spike report and increment access count."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE spike_reports
            SET access_count = access_count + 1
            WHERE id = ?
        """, (spike_id,))

        cursor.execute("""
            SELECT id, title, topic, question, findings, gotchas, resources,
                   time_invested_minutes, domain, tags, usefulness_score,
                   access_count, created_at, updated_at
            FROM spike_reports
            WHERE id = ?
        """, (spike_id,))

        spike = dict_from_row(cursor.fetchone())

        if not spike:
            raise HTTPException(status_code=404, detail="Spike report not found")

        conn.commit()

        related = []
        if spike.get("domain"):
            cursor.execute("""
                SELECT id, title, topic, usefulness_score, time_invested_minutes
                FROM spike_reports
                WHERE domain = ? AND id != ?
                ORDER BY usefulness_score DESC
                LIMIT 3
            """, (spike["domain"], spike_id))
            related = [dict_from_row(r) for r in cursor.fetchall()]

        spike["related"] = related

        return spike


@app.post("/api/spike-reports")
async def create_spike_report(spike: SpikeReportCreate) -> ActionResult:
    """Create a new spike report."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO spike_reports (
                title, topic, question, findings, gotchas, resources,
                time_invested_minutes, domain, tags, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            spike.title,
            spike.topic,
            spike.question,
            spike.findings,
            spike.gotchas,
            spike.resources,
            spike.time_invested_minutes,
            spike.domain,
            spike.tags,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))

        spike_id = cursor.lastrowid
        conn.commit()

        await manager.broadcast_update("spike_report_created", {
            "spike_id": spike_id,
            "title": spike.title,
            "topic": spike.topic,
            "domain": spike.domain
        })

        return ActionResult(
            success=True,
            message=f"Created spike report: {spike.title}",
            data={"spike_id": spike_id}
        )


@app.put("/api/spike-reports/{spike_id}")
async def update_spike_report(spike_id: int, update: SpikeReportUpdate) -> ActionResult:
    """Update an existing spike report."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("SELECT id FROM spike_reports WHERE id = ?", (spike_id,))
        if not cursor.fetchone():
            raise HTTPException(status_code=404, detail="Spike report not found")

        updates = []
        params = []

        for field in ['title', 'topic', 'question', 'findings', 'gotchas',
                      'resources', 'time_invested_minutes', 'domain', 'tags']:
            value = getattr(update, field, None)
            if value is not None:
                updates.append(f"{field} = ?")
                params.append(value)

        if not updates:
            return ActionResult(success=False, message="No updates provided")

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(spike_id)

        cursor.execute(f"""
            UPDATE spike_reports
            SET {", ".join(updates)}
            WHERE id = ?
        """, params)

        conn.commit()

        await manager.broadcast_update("spike_report_updated", {
            "spike_id": spike_id
        })

        return ActionResult(success=True, message="Spike report updated")


@app.post("/api/spike-reports/{spike_id}/rate")
async def rate_spike_report(spike_id: int, rating: SpikeReportRate) -> ActionResult:
    """Rate the usefulness of a spike report (0-5 scale)."""
    if not 0 <= rating.score <= 5:
        raise HTTPException(status_code=400, detail="Score must be between 0 and 5")

    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT usefulness_score, access_count
            FROM spike_reports WHERE id = ?
        """, (spike_id,))
        row = cursor.fetchone()

        if not row:
            raise HTTPException(status_code=404, detail="Spike report not found")

        current_score = row['usefulness_score'] or 0
        access_count = row['access_count'] or 1

        new_score = (current_score * access_count + rating.score) / (access_count + 1)

        cursor.execute("""
            UPDATE spike_reports
            SET usefulness_score = ?, updated_at = ?
            WHERE id = ?
        """, (new_score, datetime.now().isoformat(), spike_id))

        conn.commit()

        return ActionResult(
            success=True,
            message=f"Rated spike report with score {rating.score}",
            data={"new_average": round(new_score, 2)}
        )


@app.delete("/api/spike-reports/{spike_id}")
async def delete_spike_report(spike_id: int) -> ActionResult:
    """Delete a spike report."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("SELECT title FROM spike_reports WHERE id = ?", (spike_id,))
        spike = cursor.fetchone()
        if not spike:
            raise HTTPException(status_code=404, detail="Spike report not found")

        cursor.execute("DELETE FROM spike_reports WHERE id = ?", (spike_id,))
        conn.commit()

        await manager.broadcast_update("spike_report_deleted", {
            "spike_id": spike_id
        })

        return ActionResult(success=True, message=f"Deleted spike report: {spike['title']}")


@app.get("/api/queries")
async def get_queries(
    limit: int = 50,
    since: Optional[str] = None,
    domain: Optional[str] = None,
    query_type: Optional[str] = None,
    status: Optional[str] = None,
    sort_by: str = "recent"
):
    """Get building queries with optional filtering."""
    with get_db() as conn:
        cursor = conn.cursor()

        query = """
            SELECT id, query_type, session_id, agent_id, domain, tags,
                   limit_requested, max_tokens_requested, results_returned,
                   tokens_approximated, duration_ms, status, error_message,
                   error_code, golden_rules_returned, heuristics_count,
                   learnings_count, experiments_count, ceo_reviews_count,
                   query_summary, created_at, completed_at
            FROM building_queries
            WHERE 1=1
        """
        params = []

        if since:
            query += " AND created_at > ?"
            params.append(since)

        if domain:
            query += " AND domain = ?"
            params.append(domain)

        if query_type:
            query += " AND query_type = ?"
            params.append(query_type)

        if status:
            query += " AND status = ?"
            params.append(status)

        # Apply sorting
        sort_map = {
            "recent": "created_at DESC",
            "oldest": "created_at ASC",
            "slowest": "duration_ms DESC"
        }
        query += f" ORDER BY {sort_map.get(sort_by, 'created_at DESC')}"

        query += " LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/ceo-inbox")
async def get_ceo_inbox():
    """Get CEO inbox items (pending decisions)."""
    import re

    ceo_inbox_path = EMERGENT_LEARNING_PATH / "ceo-inbox"
    items = []

    if not ceo_inbox_path.exists():
        return items

    for file_path in ceo_inbox_path.glob("*.md"):
        if file_path.name == "TEMPLATE.md":
            continue

        try:
            content = file_path.read_text(encoding='utf-8')

            # Parse frontmatter-style metadata from content
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            priority_match = re.search(r'\*\*Priority:\*\*\s*(\w+)', content)
            status_match = re.search(r'\*\*Status:\*\*\s*(\w+)', content)
            date_match = re.search(r'\*\*Date:\*\*\s*([\d-]+)', content)

            # Get first paragraph after title as summary
            summary_match = re.search(r'^##\s+Context\s*\n+(.+?)(?=\n\n|\n##)', content, re.MULTILINE | re.DOTALL)
            summary = summary_match.group(1).strip()[:200] if summary_match else ""

            items.append({
                "filename": file_path.name,
                "title": title_match.group(1) if title_match else file_path.stem,
                "priority": priority_match.group(1) if priority_match else "Medium",
                "status": status_match.group(1) if status_match else "Pending",
                "date": date_match.group(1) if date_match else None,
                "summary": summary,
                "path": str(file_path)
            })
        except Exception as e:
            logger.error(f"Error reading CEO inbox item {file_path}: {e}")
            continue

    # Sort by priority (Critical > High > Medium > Low) then by date
    priority_order = {"Critical": 0, "High": 1, "Medium": 2, "Low": 3}
    items.sort(key=lambda x: (priority_order.get(x["priority"], 2), x["date"] or ""))

    return items


@app.get("/api/ceo-inbox/{filename}")
async def get_ceo_inbox_item(filename: str):
    """Get full content of a CEO inbox item."""
    # Security: validate filename
    if not re.match(r'^[\w\-]+\.md$', filename):
        raise HTTPException(status_code=400, detail="Invalid filename")

    file_path = EMERGENT_LEARNING_PATH / "ceo-inbox" / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Item not found")

    try:
        content = file_path.read_text(encoding='utf-8')
        return {"filename": filename, "content": content}
    except Exception as e:
        logger.error(f"Error reading CEO inbox item {filename}: {e}")
        raise HTTPException(status_code=500, detail="Failed to read item")


@app.get("/api/domains")
async def get_domains():
    """Get all domains with counts."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            SELECT domain, COUNT(*) as heuristic_count, AVG(confidence) as avg_confidence
            FROM heuristics
            GROUP BY domain
            ORDER BY heuristic_count DESC
        """)

        return [dict_from_row(r) for r in cursor.fetchall()]


@app.get("/api/anomalies")
async def get_anomalies():
    """Detect and return current anomalies."""
    anomalies = []

    with get_db() as conn:
        cursor = conn.cursor()

        # Repeated failures (same node failing multiple times)
        cursor.execute("""
            SELECT node_name, COUNT(*) as fail_count
            FROM node_executions
            WHERE status = 'failed'
              AND created_at > datetime('now', '-1 day')
            GROUP BY node_name
            HAVING fail_count >= 3
        """)
        for row in cursor.fetchall():
            anomalies.append({
                "type": "repeated_failure",
                "severity": "error",
                "message": f"Node '{row['node_name']}' failed {row['fail_count']} times in 24h",
                "data": {"node_name": row["node_name"], "count": row["fail_count"]}
            })

        # Sudden hot spots
        cursor.execute("""
            SELECT location, SUM(strength) as strength, COUNT(*) as count
            FROM trails
            WHERE created_at > datetime('now', '-1 day')
              AND location NOT IN (
                  SELECT DISTINCT location FROM trails
                  WHERE created_at <= datetime('now', '-1 day')
                    AND created_at > datetime('now', '-7 days')
              )
            GROUP BY location
            HAVING strength > 1.0
            ORDER BY strength DESC
            LIMIT 5
        """)
        for row in cursor.fetchall():
            anomalies.append({
                "type": "new_hotspot",
                "severity": "info",
                "message": f"New hot spot: {row['location']} ({row['count']} trails)",
                "data": {"location": row["location"], "strength": row["strength"]}
            })

        # Heuristics being violated frequently
        cursor.execute("""
            SELECT id, rule, times_violated, confidence
            FROM heuristics
            WHERE times_violated > 3 AND confidence > 0.5
            ORDER BY times_violated DESC
            LIMIT 5
        """)
        for row in cursor.fetchall():
            anomalies.append({
                "type": "heuristic_violations",
                "severity": "warning",
                "message": f"Heuristic violated {row['times_violated']}x: {row['rule'][:50]}...",
                "data": {"heuristic_id": row["id"], "violations": row["times_violated"]}
            })

        # Stale runs (running for too long)
        cursor.execute("""
            SELECT id, workflow_name, started_at
            FROM workflow_runs
            WHERE status = 'running'
              AND started_at < datetime('now', '-1 hour')
        """)
        for row in cursor.fetchall():
            anomalies.append({
                "type": "stale_run",
                "severity": "warning",
                "message": f"Run #{row['id']} ({row['workflow_name']}) has been running for >1 hour",
                "data": {"run_id": row["id"]}
            })

    return anomalies


# ==============================================================================
# REST API: Actions
# ==============================================================================

@app.post("/api/heuristics/{heuristic_id}/promote")
async def promote_to_golden(heuristic_id: int) -> ActionResult:
    """Promote a heuristic to golden rule."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("SELECT * FROM heuristics WHERE id = ?", (heuristic_id,))
        heuristic = cursor.fetchone()

        if not heuristic:
            raise HTTPException(status_code=404, detail="Heuristic not found")

        if heuristic["is_golden"]:
            return ActionResult(success=False, message="Already a golden rule")

        cursor.execute("""
            UPDATE heuristics
            SET is_golden = 1, updated_at = ?
            WHERE id = ?
        """, (datetime.now().isoformat(), heuristic_id))

        # Log the promotion
        cursor.execute("""
            INSERT INTO metrics (metric_type, metric_name, metric_value, context, timestamp)
            VALUES ('golden_rule_promotion', 'manual_promotion', ?, ?, ?)
        """, (heuristic_id, heuristic["rule"][:100], datetime.now().isoformat()))

        conn.commit()

        await manager.broadcast_update("heuristic_promoted", {
            "heuristic_id": heuristic_id,
            "rule": heuristic["rule"]
        })

        return ActionResult(success=True, message=f"Promoted to golden rule")


@app.post("/api/heuristics/{heuristic_id}/demote")
async def demote_from_golden(heuristic_id: int) -> ActionResult:
    """Demote a golden rule back to regular heuristic."""
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE heuristics
            SET is_golden = 0, updated_at = ?
            WHERE id = ?
        """, (datetime.now().isoformat(), heuristic_id))

        conn.commit()

        return ActionResult(success=True, message="Demoted from golden rule")


@app.put("/api/heuristics/{heuristic_id}")
async def update_heuristic(heuristic_id: int, update: HeuristicUpdate) -> ActionResult:
    """Update a heuristic."""
    with get_db() as conn:
        cursor = conn.cursor()

        updates = []
        params = []

        if update.rule is not None:
            updates.append("rule = ?")
            params.append(update.rule)

        if update.explanation is not None:
            updates.append("explanation = ?")
            params.append(update.explanation)

        if update.domain is not None:
            updates.append("domain = ?")
            params.append(update.domain)

        if update.is_golden is not None:
            updates.append("is_golden = ?")
            params.append(1 if update.is_golden else 0)

        if not updates:
            return ActionResult(success=False, message="No updates provided")

        updates.append("updated_at = ?")
        params.append(datetime.now().isoformat())
        params.append(heuristic_id)

        cursor.execute(f"""
            UPDATE heuristics
            SET {", ".join(updates)}
            WHERE id = ?
        """, params)

        conn.commit()

        return ActionResult(success=True, message="Heuristic updated")


@app.delete("/api/heuristics/{heuristic_id}")
async def delete_heuristic(heuristic_id: int) -> ActionResult:
    """Delete a heuristic."""
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute("DELETE FROM heuristics WHERE id = ?", (heuristic_id,))
        conn.commit()

        return ActionResult(success=True, message="Heuristic deleted")


@app.post("/api/runs/{run_id}/retry")
async def retry_run(run_id: int) -> ActionResult:
    """Retry a failed run."""
    # This would integrate with the replay.py functionality
    try:
        sys.path.insert(0, str(EMERGENT_LEARNING_PATH / "conductor"))
        from replay import ReplayManager

        replay = ReplayManager()
        new_run_id = replay.create_replay_run(run_id)

        return ActionResult(
            success=True,
            message=f"Created retry run #{new_run_id}",
            data={"new_run_id": new_run_id}
        )
    except Exception as e:
        logger.error(f"Error retrying run {run_id}: {e}", exc_info=True)
        return ActionResult(success=False, message="Failed to retry workflow run. Please try again.")


@app.post("/api/open-in-editor")
async def open_in_editor(path: str = Query(...)):
    """Open a file in VS Code."""
    try:
        # Normalize path
        file_path = Path(path).resolve()

        # SECURITY: Only allow opening files within emergent-learning directory
        allowed_root = EMERGENT_LEARNING_PATH.resolve()
        if not str(file_path).startswith(str(allowed_root)):
            return ActionResult(success=False, message="Access denied: path outside allowed directory")

        if not file_path.exists():
            return ActionResult(success=False, message="File not found")

        # SECURITY: Removed shell=True to prevent command injection
        subprocess.Popen(["code", "-g", str(file_path)])

        return ActionResult(success=True, message=f"Opened {path} in VS Code")
    except Exception as e:
        logger.error(f"Error opening file in editor: {e}", exc_info=True)
        return ActionResult(success=False, message="Failed to open file in editor. Please try again.")


# ==============================================================================
# REST API: Natural Language Query
# ==============================================================================

@app.post("/api/query")
async def natural_language_query(request: QueryRequest):
    """Natural language query interface."""
    query = request.query.lower()
    results = {
        "query": request.query,
        "heuristics": [],
        "learnings": [],
        "hotspots": [],
        "runs": [],
        "summary": ""
    }

    with get_db() as conn:
        cursor = conn.cursor()

        # Extract keywords
        keywords = re.findall(r'\b\w{3,}\b', query)
        # Escape each keyword individually before joining to prevent wildcard injection
        escaped_keywords = [escape_like(kw) for kw in keywords]
        keyword_pattern = "%".join(escaped_keywords) if escaped_keywords else "%"

        # Search heuristics
        cursor.execute("""
            SELECT id, domain, rule, confidence, times_validated
            FROM heuristics
            WHERE LOWER(rule) LIKE ? OR LOWER(domain) LIKE ? OR LOWER(explanation) LIKE ?
            ORDER BY confidence DESC
            LIMIT ?
        """, (f'%{keyword_pattern}%', f'%{keyword_pattern}%', f'%{keyword_pattern}%', request.limit))
        results["heuristics"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Search learnings
        cursor.execute("""
            SELECT id, type, title, summary, domain
            FROM learnings
            WHERE LOWER(title) LIKE ? OR LOWER(summary) LIKE ? OR LOWER(domain) LIKE ?
            ORDER BY created_at DESC
            LIMIT ?
        """, (f'%{keyword_pattern}%', f'%{keyword_pattern}%', f'%{keyword_pattern}%', request.limit))
        results["learnings"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Search hot spots
        cursor.execute("""
            SELECT location, SUM(strength) as strength, COUNT(*) as count
            FROM trails
            WHERE LOWER(location) LIKE ?
            GROUP BY location
            ORDER BY strength DESC
            LIMIT ?
        """, (f'%{keyword_pattern}%', request.limit))
        results["hotspots"] = [dict_from_row(r) for r in cursor.fetchall()]

        # Generate summary
        h_count = len(results["heuristics"])
        l_count = len(results["learnings"])
        hs_count = len(results["hotspots"])

        results["summary"] = f"Found {h_count} heuristics, {l_count} learnings, and {hs_count} hot spots matching '{request.query}'"

    return results


# ==============================================================================
# REST API: Session History
# ==============================================================================

@app.get("/api/sessions/stats")
async def get_session_stats():
    """
    Get session statistics.

    Returns:
        {
            "total_sessions": int,
            "agent_sessions": int,
            "user_sessions": int,
            "total_prompts": int,
            "last_scan": "timestamp",
            "projects_count": int
        }
    """
    try:
        stats = session_index.get_stats()
        return stats

    except Exception as e:
        logger.error(f"Error getting session stats: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to get session stats")


@app.get("/api/sessions")
async def get_sessions(
    offset: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=200),
    days: Optional[int] = Query(None, ge=1),
    project: Optional[str] = None,
    search: Optional[str] = None,
    include_agent: bool = False
):
    """
    Get list of sessions with metadata.

    Query Parameters:
        offset: Number of sessions to skip (pagination)
        limit: Maximum sessions to return (default 50, max 200)
        days: Filter to sessions from last N days
        project: Filter by project name
        search: Search in first prompt preview
        include_agent: Include agent sessions (default: False)

    Returns:
        {
            "sessions": [...],
            "total": int,
            "offset": int,
            "limit": int
        }
    """
    try:
        sessions, total = session_index.list_sessions(
            offset=offset,
            limit=limit,
            days=days,
            project=project,
            search=search,
            include_agent=include_agent
        )

        return {
            "sessions": [asdict(s) for s in sessions],
            "total": total,
            "offset": offset,
            "limit": limit
        }

    except Exception as e:
        logger.error(f"Error listing sessions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to list sessions")


@app.get("/api/sessions/{session_id}")
async def get_session(session_id: str):
    """
    Get full session content with all messages.

    Args:
        session_id: Session UUID

    Returns:
        {
            "session_id": "...",
            "project": "...",
            "project_path": "...",
            "first_timestamp": "...",
            "last_timestamp": "...",
            "prompt_count": int,
            "git_branch": "...",
            "is_agent": bool,
            "messages": [
                {
                    "uuid": "...",
                    "type": "user" | "assistant",
                    "timestamp": "...",
                    "content": "...",
                    "is_command": bool,
                    "tool_use": [...],
                    "thinking": "..."
                },
                ...
            ]
        }
    """
    try:
        session = session_index.load_full_session(session_id)

        if not session:
            raise HTTPException(status_code=404, detail="Session not found")

        return session

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error loading session {session_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to load session")


@app.get("/api/projects")
async def get_session_projects():
    """
    Get list of unique projects with session counts.

    Returns:
        [
            {
                "name": "project-name",
                "session_count": int,
                "last_activity": "timestamp"
            },
            ...
        ]
    """
    try:
        projects = session_index.get_projects()
        return projects

    except Exception as e:
        logger.error(f"Error getting projects: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to get projects")


# ==============================================================================
# REST API: Export
# ==============================================================================

@app.get("/api/export/{export_type}")
async def export_data(export_type: str, format: str = "json"):
    """Export data in various formats."""
    with get_db() as conn:
        cursor = conn.cursor()

        if export_type == "heuristics":
            cursor.execute("""
                SELECT id, domain, rule, explanation, confidence,
                       times_validated, times_violated, is_golden,
                       source_type, created_at, updated_at
                FROM heuristics
                ORDER BY confidence DESC
            """)
            data = [dict_from_row(r) for r in cursor.fetchall()]

        elif export_type == "runs":
            cursor.execute("""
                SELECT id, workflow_id, workflow_name, status, phase,
                       total_nodes, completed_nodes, failed_nodes,
                       started_at, completed_at, created_at
                FROM workflow_runs
                ORDER BY created_at DESC
            """)
            data = [dict_from_row(r) for r in cursor.fetchall()]

        elif export_type == "learnings":
            cursor.execute("""
                SELECT id, type, filepath, title, summary, domain, severity, created_at
                FROM learnings
                ORDER BY created_at DESC
            """)
            data = [dict_from_row(r) for r in cursor.fetchall()]

        elif export_type == "full":
            # Full export includes everything
            data = {
                "exported_at": datetime.now().isoformat(),
                "heuristics": [],
                "learnings": [],
                "runs": [],
                "trails": [],
                "metrics_summary": {}
            }

            cursor.execute("SELECT * FROM heuristics ORDER BY confidence DESC")
            data["heuristics"] = [dict_from_row(r) for r in cursor.fetchall()]

            cursor.execute("SELECT * FROM learnings ORDER BY created_at DESC")
            data["learnings"] = [dict_from_row(r) for r in cursor.fetchall()]

            cursor.execute("SELECT * FROM workflow_runs ORDER BY created_at DESC LIMIT 100")
            data["runs"] = [dict_from_row(r) for r in cursor.fetchall()]

            cursor.execute("""
                SELECT location, SUM(strength) as total_strength, COUNT(*) as count
                FROM trails
                GROUP BY location
                ORDER BY total_strength DESC
                LIMIT 100
            """)
            data["trails"] = [dict_from_row(r) for r in cursor.fetchall()]

            cursor.execute("""
                SELECT metric_type, COUNT(*) as count
                FROM metrics
                GROUP BY metric_type
            """)
            data["metrics_summary"] = {r["metric_type"]: r["count"] for r in cursor.fetchall()}
        else:
            raise HTTPException(status_code=400, detail=f"Unknown export type: {export_type}")

        return data


# ==============================================================================
# REST API: Workflow Management
# ==============================================================================

@app.get("/api/workflows")
async def get_workflows():
    """Get all workflow definitions."""
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT w.*, COUNT(DISTINCT we.id) as edge_count
            FROM workflows w
            LEFT JOIN workflow_edges we ON w.id = we.workflow_id
            GROUP BY w.id
            ORDER BY w.created_at DESC
        """)
        return [dict_from_row(r) for r in cursor.fetchall()]


@app.post("/api/workflows")
async def create_workflow(workflow: WorkflowCreate) -> ActionResult:
    """Create a new workflow."""
    try:
        sys.path.insert(0, str(EMERGENT_LEARNING_PATH / "conductor"))
        from conductor import Conductor

        conductor = Conductor()
        workflow_id = conductor.create_workflow(
            name=workflow.name,
            description=workflow.description,
            nodes=workflow.nodes,
            edges=workflow.edges
        )

        return ActionResult(
            success=True,
            message=f"Created workflow '{workflow.name}'",
            data={"workflow_id": workflow_id}
        )
    except Exception as e:
        logger.error(f"Error creating workflow '{workflow.name}': {e}", exc_info=True)
        return ActionResult(success=False, message="Failed to create workflow. Please check workflow configuration.")


# ==============================================================================
# Serve Frontend (Production)
# ==============================================================================

if FRONTEND_PATH.exists():
    app.mount("/assets", StaticFiles(directory=FRONTEND_PATH / "assets"), name="assets")

    @app.get("/{path:path}")
    async def serve_frontend(path: str):
        # Don't serve frontend for API paths
        if path.startswith("api/"):
            raise HTTPException(status_code=404, detail="Not found")

        file_path = FRONTEND_PATH / path
        if file_path.exists() and file_path.is_file():
            return FileResponse(file_path)
        return FileResponse(FRONTEND_PATH / "index.html")


# ==============================================================================
# Main
# ==============================================================================

if __name__ == "__main__":
    import uvicorn
    # SECURITY: Bind to localhost only - prevents exposure on public networks
    uvicorn.run(app, host="127.0.0.1", port=8888, reload=True)
